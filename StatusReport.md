Administrative Updates 

So far in the project Faris and Jon have still had the same roles and responsibilities as team members. They have both begun splitting responsibilities within the project, including writing and interpreting code. All of the project work is in Github, and the reports are being written in Markdown files. 
The 2 datasets that are still being worked with are: NBA Database and NBA Players. The NBA database includes data on all 30 NBA teams, as well as over 4,800 players and 65,000 games. There are box scores for over 95% of all of these games, as well as play-by-play game data with 13M+ rows of data. The NBA Players contains over two decades of data on each player who has been a part of an NBA roster. It includes demographic variables such as age, height, weight, place of birth, and other details such as team played for, draft year, and round. Additionally, it also contains basic box score statistics such as games played, average number of points, rebounds, and assists. Something to note is that both datasets are available on Kaggle and they are both provided under the MIT License. Because both of these datasets are licensed under the MIT License, they are both compatible for integration. This is because the MIT License is permissive and allows for reuse. 
While they initially started with 2 research questions, they have now narrowed down the scope of their project to one. The research question that they are aiming to answer is: What is the correlation between offensive rebounds and probability of winning? (In the NBA). 

Task Updates

The 4 requirements that we have completed so far are: Automatically integrating of datasets using Python/Pandas and/or SQL (weeks 5-6), Documenting data profiling, quality assessment, and cleaning (week 7), Implementing simple data analysis and/or visualization (answering at least part of your RQs), and Creating a reproducible package (week 8). We have done all of these requirements in the research.py file. Additionally, some artifacts were also created during these processes. We ended up creating a new merged dataset called merged_player_data.csv, 2 images, and also a log.txt for reproducibility. We also created a StatusReport.md markdown file to write this interim report in. 
First, we successfully integrated 2 datasets. We ensured that we were using and integrating the relevant datasets, because our datasets had a bunch of different csv files within it. However, not all of them were relevant to our research question or processes. We integrated all_seasons.csv which contained per-season player statistics and player.csv which contained player biographical information using pandas. Integrating these datasets into a new, merged dataset allowed us to perform functions on this one dataset in one, consolidated place rather than trying to perform functions across multiple datasets. 
Then, we performed data profiling using .info (), .describe (),  and uniqueness counts. We also performed quality checks for missing values and duplicate entries, as well as cleaning procedures like normalizing and standardizing names and removing duplicate entries. Profiling and cleaning the data made our functions a lot more clear and versatile. Non-normalized and standardized names meant that some data was not included in certain functions, and duplicated data also misrepresented the information that was in the dataset. 
After that, we generated 2 visualizations. The 2 visualizations we generated were: a offensive_rebound_vs_points_per_game.png which was a scatter plot of offensive rebound % vs. points per game using sns.regplot, and offensive_rebound_vs_net_rating.png which was a scatter plot of offensive rebound % vs. net rating (as a proxy for team success), color-coded by active status. We will use these 2 visualizations later on when consolidating all of our insights and analysis to answer our research question at the end of the project. 
Next, we created a reproducible python script. This script defined all of our inputs and outputs. It also saved a run log (log.txt) with metadata. We know the importance of ensuring that the project is reproducible. The project being reproducible ensures its validity and accuracy because then it can be reproduced and recreated by someone else and another location. This validates the processes and results because it is able to be done again and the same results can be achieved. 

Timeline Updates

We are still utilizing the same timeline in our project plan. We have already completed all of the administrative tasks, as well as these 4 requirements: Automatically integrating of datasets using Python/Pandas and/or SQL (weeks 5-6), Documenting data profiling, quality assessment, and cleaning (week 7), Implementing simple data analysis and/or visualization (answering at least part of your RQs), and Creating a reproducible package (week 8). With that being said, there are still tasks that will still need to be completed before the final submission. We still have to satisfy these requirements: Automating end-to-end workflow execution (week 10), accurately citing data and software used (weeks 11-12), creating metadata describing your package (week 13), and archiving the project in a repository to obtain a persistent identifier (week 14). We will look to have these tasks completed at least a week before the final deadline of May 1st so that we have buffer time for making edits and revisions before submitting. 

Project Plan Updates

There are no major project plan updates. The only major update we have is that we are narrowing the scope of our project from 2 research questions down to just 1 research question. This change makes sense to us because it allows us to tailor each of our completed requirements to answering just one research question rather than two of them. At the end of our project, we will be able to consolidate all of our findings into a concrete and tangible answer for our research question. We will aim to slowly work on the project each week until we have the rest of the requirements done and then hopefully a valid and accurate answer to our research question. We will continue splitting responsibilities and approach the rest of the project the same way that we have so far until this point. 
